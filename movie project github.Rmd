---
title: "Movie Recommendation Project"
author: "Sujata Verma"
date: "9/1/2020"
output:
  word_document: 
    fig_caption: yes
    keep_md: yes
    toc: yes
  pdf_document: default
  html_document:
    keep_md: yes
---
\newpage
#      Introduction

The ** objective** of this project is build an algorithm to predict the rating a viewer(called user) will give to a movie in the range of 0.5 (worst rating) to 5(best rating). The Movielens dataset consisting of viewer ratings of movies of variuos genres, comedy, romance etc. for the years 1995 to 2009 will be used for analysis.

I will identify the best machine learning model for predicting the user rating by fitting a few regression models to the data and calculating the Root Mean Square Error. All models with RMSE of less than 0.80 will be acceptable and the model with the lowest RMSE will be deemed best.

The key steps will be to first explore the data, data wrangling (e.g. converting timestamp into years, dividing the data set into train, test and validation sets), fitting a baseline genralized linear regression model. Lastly,the baseline model will be enhanced by fitting two additional models and to check if we get a lower RMSE.

```{r, setup, include=FALSE}
# set this option in the first code chunk in the document
knitr::opts_chunk$set(echo = FALSE,message = FALSE,warning = FALSE)
```


#  DATA WRANGLING
In this section, the data is imported and observations with  missing values are removed. The data is prepared for analysis.

The first step is to load the libraries and options.The second step is to load the Movielens data set consisting of more than 10 million ratings of 10,000 movies.

```{r}
#Creating data set
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")


dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")


```


The data set, Movielens, specifically, consists of the following variables, movie titles,genre,timestamp,userId,movieId and rating.Not all users rated all the movies, so there are lot of NA's in the dataset.Entire data matrix can be used as predictors of each cell, making it hard to predict how a user will rate a movie.

The Movielens dataset has a 'timestamp' variable about when the movie was released. It will be useful to convert it into 'year' variable for better interpretation.

```{r}

movielens<-movielens %>% mutate(year = year(as_datetime(timestamp)))

movielens<-movielens%>%select(userId,movieId,rating,title,genres,year)

movie_titles<-movielens%>%select(movieId,title)%>%distinct()

head(movielens) %>% knitr::kable()

```


Next we create a subset for data exploration and data analysis. We want to separate out 10% of data for validation and use the rest of the data for predicting. In the end, we will apply our best predictive model to the validation set to validate our results. 

The dataset movielens is split into validation set(10%) and rest 90% of the data(edx) will be further split equally between train set and test set. The edx dta set has around 9 million movie ratings, and has six variables: MovieID, userID, Movie Title, Genre, Year and Rating. Each movie can have mulitple ratings.

```{r,echo=FALSE,warnings=FALSE,message=FALSE}
set.seed(1)

test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)

edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```
Now we have two sets of data-edx and validation. In the next section, data exploration will be performed on the edx data set.

#     DATA EXPLORATION 

In this section, summary statistics will performed on the edx data set to get familiar with it.

We begin by looking at how many unique movies and users are in the edx dataset.


```{r}
edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))
```

Next we find out the most given ratings in order from most to least.

```{r}
edx %>% group_by(rating) %>%
  summarize(count = n()) %>% 
  arrange(desc(count))
```

Next we explore all the variables by looking at the summary statistics.


```{r}
summary(edx)

```

Next, the distribution of movies and users will be visualised to deepen our understanding of these variables.

  
```{r histogram for movies and users, echo=FALSE}
# histogram graph 

graph_movies <- edx %>% dplyr::count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(fill="mediumpurple3",col="grey",bins = 30, lwd=0.1) + 
  scale_x_log10() + 
  ggtitle("Movies")+xlab("Movies")+ylab("Count of ratings")

graph_users <- edx %>%
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(fill="mediumpurple3",col="grey",bins = 30, lwd=0.1) + 
  scale_x_log10() +xlab("Users")+ylab("Count of ratings")+
  ggtitle("Users")

grid.arrange(graph_movies,graph_users,ncol=2,top="Distribution of movies and users")


```

The above chart shows that some movies get rated more than others and also some users rate movies more actively than others.

#   METHOD AND ANALYSIS

In the following section, various machine learning models are built to predict the user ratings. Each model is built by using the caret package, and applying it to our edx data set. The data set is first split evenly between the training set and testing set. To make sure we don't include movies and users in the test set that don't appear in the training set, we use the semi_join function.


```{r}
#create training and testing set from edx dataset with p=0.5

set.seed(1)

options(digits=4)

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.5, list = FALSE)
train <- edx[-test_index,]
test <- edx[test_index,]

#ensure same movies and users are in both train and test sets

test <- test %>% 
  semi_join(train, by = "movieId") %>%
  semi_join(train, by = "userId")


```




In the following section,six models will be fitted. The different models are:

1. Baseline Model: Mean-only Regression Model 
2. Movie-effects Regression Model
3. Movie and User-effects Regression Model
4. Movie and User-effects Regression Model with regularization 
5. Time-effects Model
6. User Score Model

For each model, the following steps are taken:

1. Training the model using the training set
2. Making predictions about rating using the test set
3. Determining the RMSE of the predictions
4. Storing the results of the model in a results table


##  Model 1. Mean only Regression Model

The first model predicts that the user will rate the movie equal to the average of all ratings in the data set.

Define Yu,i as the rating for movie i, by user,u. μ is the mean rating. 

εu,i are the independent errors.

Thus the equation we are trying to fit is Yu,i = μ + εu,i


```{r}

#train the model
mu<-mean(train$rating)#mu is the estimated mean of all the ratings in the data set.
mu

#Calculate the RMSE:
rmse_mu<-RMSE(test$rating,mu)

#Creation of a table to store results
rmse_table <- data_frame(Model= "Mean only Model", RMSE= rmse_mu)

rmse_table %>% knitr::kable()



```
Model 1 doesn't give us the target RMSE of less than 0.80.

##  Model 2: Movie-effects Model

Adding movie-effects: Is their varibility in ratings due to differences in movies? We add the term bi to represent average rating for movie i. Now out model becomes  Yu,i = μ + bi + εu,i

```{r}
#MODEL 2:Movie effects: 

mu <- mean(train$rating) 

#Computing the bi's
movie_avgs <- train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

#predicting on the test set
predicted_ratings <- mu + test %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

#compute RMSE
rmse_movie_effects<-RMSE(predicted_ratings, test$rating)

#storing results for comparison
rmse_table <- bind_rows(rmse_table,
                      data_frame(Model= "Movie Effects Model", RMSE= rmse_movie_effects))

rmse_table %>% knitr::kable()


```

Movie effects model lowered the RMSE. We can also take into account user effects to look into the question- do different users differ in terms of how they rate movies? 
 
## Model 3: Movie and user-effects Model
 
 We add the term bu to represent average rating for user, u
So, the model we are trying to estimate becomes Yu,i = μ + bi + bu + εu,i


```{r}
#MODEL 3:Add user effects: Can variability be explained by differences in users?


#train the model and get bu's
user_avgs <- train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

#get predictions on the test set
predicted_ratings <- test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

#get RMSE
rmse_user_effects<-RMSE(predicted_ratings, test$rating)


#storing results for comparison
rmse_table <- bind_rows(rmse_table,
                        data_frame(Model= "Movie and User Effects Model", RMSE= rmse_user_effects))

rmse_table %>% knitr::kable()
```
In Model 3, the RMSE decreases significantly when we add the user effects to movie effects and mean ratings.In the next model, we use the process of 'regularization' to remove the disproportionate effects of movies with few ratings and users with few ratings by adding a penalty variable, lambda.

##  Models 4 : Movie and user effects model with regularization 

We regularize the previous model, with optimized lambda to prevent overfitting.

```{r}
# Optimizing for lambda

lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train$rating)
  
  b_i <- train %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- 
    test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test$rating))
})


#Finding the best value of lambda
qplot(lambdas, rmses)  
lambda <- lambdas[which.min(rmses)]
lambda



```

Next, we repaet Model 4 with the optimized value of lambda found in the previous section.

```{r}
#repeat model 4 with optimal lambda value 

#training the model
mu<-mean(train$rating)
movie_reg_avgs<-train%>%group_by(movieId)%>%
  summarize(b_i=sum(rating-mu)/(n()+lambda),n_i=n())

user_reg_avgs <- train %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - mu - b_i)/(n()+lambda),n_u=n())

#predicting the model
predicted_ratings <- test %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(user_reg_avgs, by = "userId") %>%
  mutate(pred = mu + b_i+b_u) %>%
  pull(pred)

#calculating RMSE
rmse_regularized_effects<-RMSE(predicted_ratings, test$rating)

#storing results for comparison
rmse_table <- bind_rows(rmse_table,
                        data_frame(Model= "Regularized Movie and User Effects Model", RMSE= rmse_regularized_effects))

rmse_table %>% knitr::kable()

```



Model 4 did reduce the RMSE a little but did not significantly reduce it. So, instead of regularization, we will experiment with adding the time effects in the next model to explore if adding a time variable explains the variations in ratings. We add bt to capture the effect of the year on explaining the varibility in ratingst, so our equation now becomes  Yu,i = μ + bi + bu + bt + εu,i,t


## Model 5: Time-effects Model

Add time effects to the movie and user-effects model (no regularization)

```{r}

#Add time effects:

mu <- mean(train$rating) 
time_avgs <- train %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs,by="userId")%>%
  group_by(year) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u))

#get predictions and rmse

predicted_ratings <- test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(time_avgs, by='year')%>%
  mutate(pred = mu + b_i + b_u + b_t) %>%
  pull(pred)

#calculating RMSE
rmse_time_effects<-RMSE(predicted_ratings, test$rating)

#storing results for comparison
rmse_table <- bind_rows(rmse_table,
                        data_frame(Model= "Time Effects Model", RMSE= rmse_time_effects))

rmse_table %>% knitr::kable()

```


Adding the time effects to the movie and user effects model(without regularization) and then estimating the RMSE resulted in RMSE of 0.8697. It didn't perform better than the movie and user effects model with regularization, thus I didn't pursue it further. 

## Model 6: User Score Model

I tried another model and called it User Score Model, a combination of movie-user effects. It included the following steps:

A.Since the training set was very large, I subsetted it to include 500 movies with most ratings and 500 most prolific users to fit the models.Similarly, the test set was subsetted to include the same movieId and userId as the training subset. Note that the two subsets have different number of observations though the variables are the same, userId, movieId and rating. 

```{r}
train_subset<-train %>% group_by(movieId)%>%filter(n()>=500)%>% ungroup()%>%
group_by(userId)%>%filter(n()>=500)%>%ungroup()%>%
select(userId,movieId,rating)

 test_subset<-test%>%select(userId,movieId,rating)%>%semi_join(train_subset, by = "movieId") %>% semi_join(train_subset, by = "userId")
 
dim(train_subset)
dim(test_subset)
identical(test_subset,train_subset)
 
```

B. I calculated the average rating for all the movies in the train-subset.First, I converted the train_subset into a matrix,train_mat then found out the column means,col_means. These are the average ratings for all of the movies in the data base. Next step is to subtract the average movie rating from each of the user ratings to give us the divergence matrix. There are a number of NA's since not each user rated each movie. Each value in the divergence matrix gives us how a user rating diverged from the average rating of the movie.

```{r}
#Let us explore the training subset in more detail and create an algorithm
#Goal:create a user score for each unique user.
#First step is to convert the training subset into a matrix.

train_mat<-train_subset%>%spread(movieId,rating)%>%as.matrix()

#Second step is to calculate the average rating of every movie in the training subset
#by calculating column means and then subtract each user's actual rating
#by the column mean to see how the rating diverged.

r_mat<-train_mat[,-1]#remove column headings
col_means<-colMeans(r_mat,na.rm=TRUE)#calculate column means or average movie ratings
diverge<-sweep(r_mat,2,colMeans(r_mat,na.rm=TRUE))#subtract user rating by average movie rating


print(diverge[1:6,1:6])#gives how the user rating diverged from the average movie rating


```

C. For each user,u, I calculated the divergence of their rating from the average movie rating (ignoring NA's) of a movie,i, as shown above. The next step was to calculate the mean of this divergence among all movies each user had rated. I called it the score. Next, I combined the userId with their score. Below is the userId in the first column and their user score in the second column.

```{r}
#Third step is calculate the rowMeans to show how the user ratings differred across all the movies that they rated previously.This is the user-score. 

score<-as.matrix(rowMeans(diverge,na.rm=TRUE))

#Next,we combine the userId with the respective user-score

user_score<-cbind(train_mat[,1],score)

head(user_score) %>%knitr::kable()
```

D. Now we can predict the rating a user will give any movie. We can simply add their user score to the average rating for that movie. More specifically, the prediction of user u for movie i is computed by adding the user-score, given by variable 'score' to the average of movie,i.


E.Let us take a specific example. Let us see how the model would predict all the missing ratings within the data set for userId 1860, our first user in the train subset. We add back the movie titles, given in the first column below. The column means gives us the average rating for that movie.since we know that this user
diverges from average movie score by -0.1713 (from row 1,colmn 1 of the user-score matrix),we can predict rating for each movie by subtracting it from movie averages for all movies. In column 3 gives the rating user 1860 would have given each movie.

```{r}

#Create a matrix y, with movie titles:
options(digits=3)

y <- train_subset %>% 
  spread(movieId, rating) %>%
  as.matrix()

rownames(y)<- y[,1]
y <- y[,-1]
colnames(y) <- with(movie_titles, title[match(colnames(y), movieId)])

#Combine the movie titles with averages movie ratings for all movies

col_means<-round(col_means,digits=3)
m<-cbind(colnames(y),col_means)

#predicting ratings given by userId 1860 
user_1860_rating<-col_means-0.1713
pred_user_1860<-cbind(m,user_1860_rating)
head(pred_user_1860)


```

F.Next step is to find out the RMSE for the test-subset.It is important to note that the user-scores and prediction was done separately within the testing model and RMSE was calculated. The reason is that training and testing subsets differ in the movie-user combination as we saw earlier they are not identical in dimensions.So next, we apply the User Score Model to the test set and calculate the RMSE

```{r}
score_avgs_test<-test_subset%>%group_by(movieId)%>%
   mutate(m=mean(rating),na.rm=TRUE)%>%ungroup()%>%
   group_by(userId)%>%
   mutate(s=mean(rating-m),na.rm=TRUE)%>%ungroup()%>%mutate(pred=m+s)
 
 rmse_test_subset<-RMSE(score_avgs_test$pred,score_avgs_test$rating)
 
 #storing results for comparison
rmse_table <- bind_rows(rmse_table,
                        data_frame(Model= "User Score Model", RMSE= rmse_test_subset))

rmse_table %>% knitr::kable()
```

From the above table, we can see that this model gives us the lowest RMSE and it below our target of 0.80. Thus we will apply this model to the validation set.

G.Lastly, we apply the score-model to the validation set and calculated the RMSE.  The important caveat being that training subset and validation set had different movies and users, so the prediction had to be created within the validation set for all the movies and users.

```{r}


 #Apply the score-model to find the rmse
 
 score_avgs_validation<-validation%>%group_by(movieId)%>%
   mutate(m=mean(rating),na.rm=TRUE)%>%ungroup()%>%
   group_by(userId)%>%
   mutate(s=mean(rating-m),na.rm=TRUE)%>%ungroup()%>%mutate(pred=m+s)
 
 
 rmse_validation_subset<-RMSE(score_avgs_validation$pred,score_avgs_validation$rating)
 #storing results for comparison
rmse_table <- bind_rows(rmse_table,
                        data_frame(Model= "Validation User Score Model", RMSE= rmse_validation_subset))

rmse_table %>% knitr::kable()
```




 #   RESULTS
 
Summary of Results:

```{r}
rmse_table %>% knitr::kable()
```

 
 
From the results table, we note that there was a large decrease in RMSE betwen movie-effects only(Model 2) and user-movie effects(Model 3), showing that characteristics of users were the most important factor in predicting ratings.

The best results were obtained by the User score Model (Model 6) since it had the lowest RMSE. This was validated using the validation set.


#       CONCLUSIONS,LIMITATIONS AND RECOMMENDATIONS 

I will discuss the two extensions I added to enhance the baseline model.

The Time-effects Model asked if the variation in year given in the dataset had any significant effect on how users rated the movies.No significant effect was found as the RMSE remained exactly the same as the movie and user effects. This is probably due to the short timespan 1995-2009. A further enhancement can be to add the regularization model and check if we get a lower RMSE. 

I formulated the User Score algorithm, but it was hard to test in the conventional way. The idea behind the user score-method is intutive. 

How would a particular user rate a particular movie? 
Each movie has an average rating based of millions of viewers and can be assigned a score. We can see how a particular users rating pattern is by calculating the divergence of their rating from the average movie rating from prior rating history and derive a user-score. If the user is very discerning, when everyone gives a 4 and they give a 3, their score is (-1)then for any movie we can predict they will rate it one star below the average movie rating.

Limitation of the model was that the predictions had to be created within train_subset, test_subset and validation dataset, leading to overfitting.

As an extension,I would get the average movie rating from another data source, not movielens, and then calculate the user-score and do the predictions within this model.

Other extensions could be finding similarity within users about their rating history and modifying this prediction to include these clusters.


 

